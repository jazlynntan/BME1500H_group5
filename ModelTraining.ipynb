{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureExtraction_v2 import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/jazlynn/Downloads/neurons-smr-format-sorted/'\n",
    "\n",
    "results_dict = {'Filename': [],\n",
    "                'firing rate': [],\n",
    "                'ifr_mean': [],\n",
    "                'ifr_skew': [],\n",
    "                'ifr_kurtosis': [],\n",
    "                'ifr_fano_factor': [],\n",
    "                'ISI_mean': [],\n",
    "                'ISI_skew': [],\n",
    "                'ISI_kurtosis': [],\n",
    "                'ISI_cv': [],\n",
    "                'num_bursts': [], \n",
    "                'mean_surprise': [],\n",
    "                'burst_index': [],\n",
    "                'mean_burst_duration': [],\n",
    "                'var_burst_duration': [],\n",
    "                'mean_interburst_duration': [],\n",
    "                'var_interburst_duration': [],\n",
    "                'max_peak_freq': [],\n",
    "                'delta_band': [],\n",
    "                'theta_band': [],\n",
    "                'alpha_band': [],\n",
    "                'beta_band': [],\n",
    "                'gamma_band': []}\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.smr'):\n",
    "        analogsignal, spike_times, sampling_frequency, time = load_spiketrain(os.path.join(data_dir,file))\n",
    "        results_dict['Filename'].append(file)\n",
    "        \n",
    "        results_dict['firing rate'].append(get_firing_rate(spike_times, analogsignal, sampling_frequency))\n",
    "        ifr_ls, time_bins = calculate_instantaneous_firing_rate(spike_times, analogsignal, sampling_frequency, 0.05, 0.1)\n",
    "        fano_factor, ifr_mean, ifr_skew, ifr_kurtosis = get_ifr_metrics(ifr_ls)\n",
    "        results_dict['ifr_mean'].append(ifr_mean)\n",
    "        results_dict['ifr_skew'].append(ifr_skew)\n",
    "        results_dict['ifr_kurtosis'].append(ifr_kurtosis)\n",
    "        results_dict['ifr_fano_factor'].append(fano_factor)\n",
    "        \n",
    "        ISI_cv, ISI_skew, ISI_kurtosis, ISI_mean, ISI_mode = get_ISI_metrics(spike_times)\n",
    "        results_dict['ISI_cv'].append(ISI_cv)\n",
    "        results_dict['ISI_skew'].append(ISI_skew)\n",
    "        results_dict['ISI_kurtosis'].append(ISI_kurtosis)\n",
    "        results_dict['ISI_mean'].append(ISI_mean)\n",
    "        \n",
    "        burst_dict = burst_detection_neuroexplorer(spike_times, \n",
    "                                                   analogsignal, \n",
    "                                                   sampling_frequency, \n",
    "                                                   min_surprise = 5, \n",
    "                                                   min_numspikes = 3)\n",
    "        num_bursts, mean_surprise, burst_index_poisson, mean_burst_duration, var_burst_duration, mean_interburst_duration, var_interburst_duration = get_burst_metrics(burst_dict, spike_times)\n",
    "        burst_index = get_burst_index(spike_times)\n",
    "        results_dict['num_bursts'].append(num_bursts)\n",
    "        results_dict['mean_surprise'].append(mean_surprise)\n",
    "        results_dict['burst_index'].append(burst_index) # not using poisson surprise\n",
    "        results_dict['mean_burst_duration'].append(mean_burst_duration)\n",
    "        results_dict['var_burst_duration'].append(var_burst_duration)\n",
    "        results_dict['mean_interburst_duration'].append(mean_interburst_duration)\n",
    "        results_dict['var_interburst_duration'].append(var_interburst_duration)\n",
    "        \n",
    "        max_peak_freq, freq_peaks, peak_bands, freq_magnitude = get_synchrony_features2(spike_times, \n",
    "                                                                                       time_bin_size = 0.01, \n",
    "                                                                                       max_lag_time = 0.5, \n",
    "                                                                                       significance_level = 0.05, \n",
    "                                                                                       to_plot = False)\n",
    "        results_dict['max_peak_freq'].append(max_peak_freq)\n",
    "        results_dict['delta_band'].append(int('delta' in peak_bands))\n",
    "        results_dict['theta_band'].append(int('theta' in peak_bands))\n",
    "        results_dict['alpha_band'].append(int('alpha' in peak_bands))\n",
    "        results_dict['beta_band'].append(int('beta' in peak_bands))\n",
    "        results_dict['gamma_band'].append(int('gamma' in peak_bands))\n",
    "        \n",
    "        \n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df.head()\n",
    "results_df.to_csv('ExtractedFeatures_v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('/Users/jazlynn/Downloads/bme1500-project-5-metadata.csv')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('ExtractedFeatures_v3.csv')\n",
    "\n",
    "print(results_df.isnull().any())\n",
    "results_df.fillna(0,inplace=True)\n",
    "\n",
    "combined_data = pd.merge(metadata, results_df,on='Filename')\n",
    "combined_data['type'] = combined_data['Neuron'] + '_' + combined_data['Hemisphere']\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_left_STN = combined_data.loc[(combined_data['Hemisphere']=='L') & (combined_data['Target']=='STN')]\n",
    "subset_left_GPi = combined_data.loc[(combined_data['Hemisphere']=='L') & (combined_data['Target']=='GPi')]\n",
    "subset_right_STN = combined_data.loc[(combined_data['Hemisphere']=='R') & (combined_data['Target']=='STN')]\n",
    "subset_right_GPi = combined_data.loc[(combined_data['Hemisphere']=='R') & (combined_data['Target']=='GPi')]\n",
    "\n",
    "print('left STN', len(combined_data.loc[(combined_data['Hemisphere']=='L') & (combined_data['Neuron']=='STN')]))\n",
    "print('right STN', len(combined_data.loc[(combined_data['Hemisphere']=='R') & (combined_data['Neuron']=='STN')]))\n",
    "print('left SNr', len(combined_data.loc[(combined_data['Hemisphere']=='L') & (combined_data['Neuron']=='SNr')]))\n",
    "print('right SNr', len(combined_data.loc[(combined_data['Hemisphere']=='R') & (combined_data['Neuron']=='SNr')]))\n",
    "print('left HFD', len(combined_data.loc[(combined_data['Hemisphere']=='L') & (combined_data['Neuron']=='HFD')]))\n",
    "print('right HFD', len(combined_data.loc[(combined_data['Hemisphere']=='R') & (combined_data['Neuron']=='HFD')]))\n",
    "print('left BOR', len(combined_data.loc[(combined_data['Hemisphere']=='L') & (combined_data['Neuron']=='BOR')]))\n",
    "print('right BOR', len(combined_data.loc[(combined_data['Hemisphere']=='R') & (combined_data['Neuron']=='BOR')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = combined_data.columns\n",
    "features = features.drop(['Filename','Unnamed: 0', 'Patient ID', 'Target', 'Neuron','Hemisphere','firing rate','type'])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for population in [subset_left_GPi,subset_right_GPi,subset_left_STN,subset_right_STN]:\n",
    "    print('Classifying region ' + population['Target'].iloc[0] + ' hemi ' + population['Hemisphere'].iloc[0])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(population[features],\n",
    "                                                        population['Neuron'],\n",
    "                                                        stratify=population['Neuron'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "    RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    RF.fit(X_train, y_train)\n",
    "    train_pred = RF.predict(X_train)\n",
    "    test_pred = RF.predict(X_test)\n",
    "    print('train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('test accuracy', accuracy_score(y_test,test_pred))\n",
    "    # cm = confusion_matrix(y_test, test_pred, labels=RF.classes_)\n",
    "    # ConfusionMatrixDisplay.from_estimator(RF,X_test, y_test, labels=RF.classes_,cmap='gray')\n",
    "    \n",
    "    feature_importance = pd.DataFrame({'feature': RF.feature_names_in_, 'importance':RF.feature_importances_})\n",
    "    feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n",
    "    # fig = sns.barplot(data=feature_importance,x='feature',y='importance')\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.savefig('feature_importance_' + population['Target'].iloc[0] + ' hemi ' + population['Hemisphere'].iloc[0] + '.svg')\n",
    "    \n",
    "    \n",
    "    pruned_features = feature_importance['feature'].iloc[:4].tolist()\n",
    "    print('selected features', pruned_features)\n",
    "    \n",
    "    RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    RF.fit(X_train[pruned_features], y_train)\n",
    "    train_pred = RF.predict(X_train[pruned_features])\n",
    "    test_pred = RF.predict(X_test[pruned_features])\n",
    "    print('pruned train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('pruned test accuracy', accuracy_score(y_test,test_pred))\n",
    "    \n",
    "    parameters = {'n_estimators':[100,200,400], 'max_depth': [5,10,None],'min_samples_leaf': [0.001,0.01,0.1],'min_samples_split': [0.001,0.01,0.1]}\n",
    "    RF_pruned = RandomForestClassifier(random_state=42)\n",
    "    hypparam_opt = GridSearchCV(RF_pruned,parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "    hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "    \n",
    "    opt_train_pred = hypparam_opt.predict(X_train[pruned_features])\n",
    "    opt_test_pred = hypparam_opt.predict(X_test[pruned_features])\n",
    "    cm = confusion_matrix(y_test, opt_test_pred, labels=hypparam_opt.classes_)\n",
    "    ConfusionMatrixDisplay.from_estimator(hypparam_opt,X_test[pruned_features], y_test, labels=hypparam_opt.classes_,cmap='gray')\n",
    "    plt.savefig('Confusion_' + population['Target'].iloc[0] + ' hemi ' + population['Hemisphere'].iloc[0] + '.svg')\n",
    "    print('optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "    print('optimized test accuracy', accuracy_score(y_test,opt_test_pred))\n",
    "    \n",
    "    fig,ax=plt.subplots()\n",
    "    sns.barplot(feature_importance,x='feature',y='importance',ax=ax)\n",
    "    ax.set_ylabel('Gini importance')\n",
    "    ax.tick_params(axis='x', labelrotation = 90)\n",
    "    \n",
    "    ### baseline model ###\n",
    "    print('baseline logistic regression optimized features')\n",
    "    LR = LogisticRegression(random_state=42, max_iter=150,penalty=None)\n",
    "    LR.fit(X_train[pruned_features], y_train)\n",
    "    train_pred = LR.predict(X_train[pruned_features])\n",
    "    test_pred = LR.predict(X_test[pruned_features])\n",
    "    print('baseline train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('baseline test accuracy', accuracy_score(y_test,test_pred))\n",
    "    \n",
    "    ### SVM ###\n",
    "    print('SVM model')\n",
    "    print('SVM before optimization')\n",
    "    svm = SVC(gamma='auto',random_state=42)\n",
    "    svm.fit(X_train[pruned_features], y_train)\n",
    "    train_pred = svm.predict(X_train[pruned_features])\n",
    "    test_pred = svm.predict(X_test[pruned_features])\n",
    "    \n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(gamma='auto',random_state=42))])\n",
    "    pipe.fit(X_train[pruned_features], y_train)\n",
    "    train_pred = pipe.predict(X_train[pruned_features])\n",
    "    test_pred = pipe.predict(X_test[pruned_features])\n",
    "    \n",
    "    print('svm train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('svm test accuracy', accuracy_score(y_test,test_pred))\n",
    "    \n",
    "    svm_parameters = {'svc__C': [0.1, 1, 10], 'svc__gamma': [1, 0.1, 0.01, 0.001]}\n",
    "    # SVM_pruned = SVC(random_state=42,kernel='rbf')\n",
    "    # SVM_hypparam_opt = GridSearchCV(SVM_pruned,svm_parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "    # SVM_hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "    \n",
    "    pipe_pruned = Pipeline([('scaler', StandardScaler()), ('svc', SVC(random_state=42))])\n",
    "    SVM_hypparam_opt = GridSearchCV(pipe_pruned,svm_parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "    SVM_hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "    \n",
    "    opt_train_pred = SVM_hypparam_opt.predict(X_train[pruned_features])\n",
    "    opt_test_pred = SVM_hypparam_opt.predict(X_test[pruned_features], pos_label=population['Neuron'].iloc[0])\n",
    "    print('SVM optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "    print('SVM optimized test accuracy', accuracy_score(y_test,opt_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if dont split hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_STN = combined_data.loc[(combined_data['Target']=='STN')]\n",
    "subset_GPi = combined_data.loc[(combined_data['Target']=='GPi')]\n",
    "\n",
    "for population in [subset_GPi,subset_STN]:\n",
    "    print('Classifying region ' + population['Target'].iloc[0] + ' hemi ' + population['Hemisphere'].iloc[0])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(population[features],\n",
    "                                                        population['Neuron'],\n",
    "                                                        stratify=population['Neuron'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "    RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    RF.fit(X_train, y_train)\n",
    "    train_pred = RF.predict(X_train)\n",
    "    test_pred = RF.predict(X_test)\n",
    "    print('train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('test accuracy', accuracy_score(y_test,test_pred))\n",
    "    cm = confusion_matrix(y_test, test_pred, labels=RF.classes_)\n",
    "    ConfusionMatrixDisplay.from_estimator(RF,X_test, y_test, labels=RF.classes_,cmap='gray')\n",
    "    \n",
    "    feature_importance = pd.DataFrame({'feature': RF.feature_names_in_, 'importance':RF.feature_importances_})\n",
    "    feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n",
    "    \n",
    "    pruned_features = feature_importance['feature'].iloc[:4].tolist()\n",
    "    print('selected features', pruned_features)\n",
    "    \n",
    "    RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    RF.fit(X_train[pruned_features], y_train)\n",
    "    train_pred = RF.predict(X_train[pruned_features])\n",
    "    test_pred = RF.predict(X_test[pruned_features])\n",
    "    print('pruned train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('pruned test accuracy', accuracy_score(y_test,test_pred))\n",
    "    \n",
    "    parameters = {'n_estimators':[100,200,400], 'max_depth': [5,10,None],'min_samples_leaf': [0.001,0.01,0.1],'min_samples_split': [0.001,0.01,0.1]}\n",
    "    RF_pruned = RandomForestClassifier(random_state=42)\n",
    "    hypparam_opt = GridSearchCV(RF_pruned,parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "    hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "    \n",
    "    opt_train_pred = hypparam_opt.predict(X_train[pruned_features])\n",
    "    opt_test_pred = hypparam_opt.predict(X_test[pruned_features])\n",
    "    print('optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "    print('optimized test accuracy', accuracy_score(y_test,opt_test_pred))\n",
    "    \n",
    "    # fig,ax=plt.subplots()\n",
    "    # sns.barplot(feature_importance,x='feature',y='importance',ax=ax)\n",
    "    # ax.set_ylabel('Gini importance')\n",
    "    # ax.tick_params(axis='x', labelrotation = 90)\n",
    "    \n",
    "    ### baseline model ###\n",
    "    \n",
    "    print('baseline logistic regression optimized features')\n",
    "    LR = LogisticRegression(random_state=42, max_iter=150,penalty=None)\n",
    "    LR.fit(X_train[pruned_features], y_train)\n",
    "    train_pred = LR.predict(X_train[pruned_features])\n",
    "    test_pred = LR.predict(X_test[pruned_features])\n",
    "    print('baseline train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('baseline test accuracy', accuracy_score(y_test,test_pred))\n",
    "    \n",
    "    ### SVM ###\n",
    "    print('SVM model')\n",
    "    print('SVM before optimization')\n",
    "    svm = SVC(gamma='auto',random_state=42)\n",
    "    svm.fit(X_train[pruned_features], y_train)\n",
    "    train_pred = svm.predict(X_train[pruned_features])\n",
    "    test_pred = svm.predict(X_test[pruned_features])\n",
    "    print('svm train accuracy', accuracy_score(y_train,train_pred))\n",
    "    print('svm test accuracy', accuracy_score(y_test,test_pred))\n",
    "    \n",
    "    svm_parameters = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "    SVM_pruned = SVC(random_state=42,kernel='rbf')\n",
    "    SVM_hypparam_opt = GridSearchCV(SVM_pruned,svm_parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "    SVM_hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "    \n",
    "    opt_train_pred = SVM_hypparam_opt.predict(X_train[pruned_features])\n",
    "    opt_test_pred = SVM_hypparam_opt.predict(X_test[pruned_features])\n",
    "    print('SVM optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "    print('SVM optimized test accuracy', accuracy_score(y_test,opt_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### can left side generalise to right and vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train left GPi')\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset_left_GPi[features],\n",
    "                                                        subset_left_GPi['Neuron'],\n",
    "                                                        stratify=subset_left_GPi['Neuron'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train, y_train)\n",
    "train_pred = RF.predict(X_train)\n",
    "test_pred = RF.predict(X_test)\n",
    "print('train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('test accuracy', accuracy_score(y_test,test_pred))\n",
    "# cm = confusion_matrix(y_test, test_pred, labels=RF.classes_)\n",
    "# ConfusionMatrixDisplay.from_estimator(RF,X_test, y_test, labels=RF.classes_,cmap='gray')\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': RF.feature_names_in_, 'importance':RF.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n",
    "\n",
    "pruned_features = feature_importance['feature'].iloc[:4].tolist()\n",
    "print('selected features', pruned_features)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train[pruned_features], y_train)\n",
    "train_pred = RF.predict(X_train[pruned_features])\n",
    "test_pred = RF.predict(X_test[pruned_features])\n",
    "print('pruned train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('pruned test accuracy', accuracy_score(y_test,test_pred))\n",
    "\n",
    "parameters = {'n_estimators':[100,200,400], 'max_depth': [5,10,None],'min_samples_leaf': [0.001,0.01,0.1],'min_samples_split': [0.001,0.01,0.1]}\n",
    "RF_pruned = RandomForestClassifier(random_state=42)\n",
    "hypparam_opt = GridSearchCV(RF_pruned,parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "\n",
    "opt_train_pred = hypparam_opt.predict(X_train[pruned_features])\n",
    "opt_test_pred = hypparam_opt.predict(X_test[pruned_features])\n",
    "print('optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "print('optimized test accuracy', accuracy_score(y_test,opt_test_pred))\n",
    "\n",
    "right_pred = hypparam_opt.predict(subset_right_GPi[pruned_features])\n",
    "print('right accuracy', accuracy_score(subset_right_GPi['Neuron'],right_pred))\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "# sns.barplot(feature_importance,x='feature',y='importance',ax=ax)\n",
    "# ax.set_ylabel('Gini importance')\n",
    "# ax.tick_params(axis='x', labelrotation = 90)\n",
    "\n",
    "print('train left STN')\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset_left_STN[features],\n",
    "                                                        subset_left_STN['Neuron'],\n",
    "                                                        stratify=subset_left_STN['Neuron'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train, y_train)\n",
    "train_pred = RF.predict(X_train)\n",
    "test_pred = RF.predict(X_test)\n",
    "print('train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('test accuracy', accuracy_score(y_test,test_pred))\n",
    "# cm = confusion_matrix(y_test, test_pred, labels=RF.classes_)\n",
    "# ConfusionMatrixDisplay.from_estimator(RF,X_test, y_test, labels=RF.classes_,cmap='gray')\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': RF.feature_names_in_, 'importance':RF.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n",
    "\n",
    "pruned_features = feature_importance['feature'].iloc[:4].tolist()\n",
    "print('selected features', pruned_features)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train[pruned_features], y_train)\n",
    "train_pred = RF.predict(X_train[pruned_features])\n",
    "test_pred = RF.predict(X_test[pruned_features])\n",
    "print('pruned train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('pruned test accuracy', accuracy_score(y_test,test_pred))\n",
    "\n",
    "parameters = {'n_estimators':[100,200,400], 'max_depth': [5,10,None],'min_samples_leaf': [0.001,0.01,0.1],'min_samples_split': [0.001,0.01,0.1]}\n",
    "RF_pruned = RandomForestClassifier(random_state=42)\n",
    "hypparam_opt = GridSearchCV(RF_pruned,parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "\n",
    "opt_train_pred = hypparam_opt.predict(X_train[pruned_features])\n",
    "opt_test_pred = hypparam_opt.predict(X_test[pruned_features])\n",
    "print('optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "print('optimized test accuracy', accuracy_score(y_test,opt_test_pred))\n",
    "\n",
    "right_pred = hypparam_opt.predict(subset_right_STN[pruned_features])\n",
    "print('right accuracy', accuracy_score(subset_right_STN['Neuron'],right_pred))\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "# sns.barplot(feature_importance,x='feature',y='importance',ax=ax)\n",
    "# ax.set_ylabel('Gini importance')\n",
    "# ax.tick_params(axis='x', labelrotation = 90)\n",
    "\n",
    "print('train right GPi')\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset_right_GPi[features],\n",
    "                                                        subset_right_GPi['Neuron'],\n",
    "                                                        stratify=subset_right_GPi['Neuron'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train, y_train)\n",
    "train_pred = RF.predict(X_train)\n",
    "test_pred = RF.predict(X_test)\n",
    "print('train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('test accuracy', accuracy_score(y_test,test_pred))\n",
    "# cm = confusion_matrix(y_test, test_pred, labels=RF.classes_)\n",
    "# ConfusionMatrixDisplay.from_estimator(RF,X_test, y_test, labels=RF.classes_,cmap='gray')\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': RF.feature_names_in_, 'importance':RF.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n",
    "\n",
    "pruned_features = feature_importance['feature'].iloc[:4].tolist()\n",
    "print('selected features', pruned_features)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train[pruned_features], y_train)\n",
    "train_pred = RF.predict(X_train[pruned_features])\n",
    "test_pred = RF.predict(X_test[pruned_features])\n",
    "print('pruned train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('pruned test accuracy', accuracy_score(y_test,test_pred))\n",
    "\n",
    "parameters = {'n_estimators':[100,200,400], 'max_depth': [5,10,None],'min_samples_leaf': [0.001,0.01,0.1],'min_samples_split': [0.001,0.01,0.1]}\n",
    "RF_pruned = RandomForestClassifier(random_state=42)\n",
    "hypparam_opt = GridSearchCV(RF_pruned,parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "\n",
    "opt_train_pred = hypparam_opt.predict(X_train[pruned_features])\n",
    "opt_test_pred = hypparam_opt.predict(X_test[pruned_features])\n",
    "print('optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "print('optimized test accuracy', accuracy_score(y_test,opt_test_pred))\n",
    "\n",
    "left_pred = hypparam_opt.predict(subset_left_GPi[pruned_features])\n",
    "print('left accuracy', accuracy_score(subset_left_GPi['Neuron'],left_pred))\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "# sns.barplot(feature_importance,x='feature',y='importance',ax=ax)\n",
    "# ax.set_ylabel('Gini importance')\n",
    "# ax.tick_params(axis='x', labelrotation = 90)\n",
    "\n",
    "print('train right STN')\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset_right_STN[features],\n",
    "                                                        subset_right_STN['Neuron'],\n",
    "                                                        stratify=subset_right_STN['Neuron'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train, y_train)\n",
    "train_pred = RF.predict(X_train)\n",
    "test_pred = RF.predict(X_test)\n",
    "print('train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('test accuracy', accuracy_score(y_test,test_pred))\n",
    "# cm = confusion_matrix(y_test, test_pred, labels=RF.classes_)\n",
    "# ConfusionMatrixDisplay.from_estimator(RF,X_test, y_test, labels=RF.classes_,cmap='gray')\n",
    "\n",
    "feature_importance = pd.DataFrame({'feature': RF.feature_names_in_, 'importance':RF.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n",
    "\n",
    "pruned_features = feature_importance['feature'].iloc[:4].tolist()\n",
    "print('selected features', pruned_features)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train[pruned_features], y_train)\n",
    "train_pred = RF.predict(X_train[pruned_features])\n",
    "test_pred = RF.predict(X_test[pruned_features])\n",
    "print('pruned train accuracy', accuracy_score(y_train,train_pred))\n",
    "print('pruned test accuracy', accuracy_score(y_test,test_pred))\n",
    "\n",
    "parameters = {'n_estimators':[100,200,400], 'max_depth': [5,10,None],'min_samples_leaf': [0.001,0.01,0.1],'min_samples_split': [0.001,0.01,0.1]}\n",
    "RF_pruned = RandomForestClassifier(random_state=42)\n",
    "hypparam_opt = GridSearchCV(RF_pruned,parameters,cv=5,scoring='accuracy',refit=True,n_jobs=4)\n",
    "hypparam_opt.fit(X_train[pruned_features],y_train)\n",
    "\n",
    "opt_train_pred = hypparam_opt.predict(X_train[pruned_features])\n",
    "opt_test_pred = hypparam_opt.predict(X_test[pruned_features])\n",
    "print('optimized train accuracy', accuracy_score(y_train,opt_train_pred))\n",
    "print('optimized test accuracy', accuracy_score(y_test,opt_test_pred))\n",
    "\n",
    "left_pred = hypparam_opt.predict(subset_left_STN[pruned_features])\n",
    "print('left accuracy', accuracy_score(subset_left_STN['Neuron'],left_pred))\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "# sns.barplot(feature_importance,x='feature',y='importance',ax=ax)\n",
    "# ax.set_ylabel('Gini importance')\n",
    "# ax.tick_params(axis='x', labelrotation = 90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,population in enumerate([subset_left_GPi,subset_right_GPi,subset_left_STN,subset_right_STN]):\n",
    "    sns.pairplot(population[['ISI_mean', 'ifr_mean', 'mean_surprise','Neuron']],hue='Neuron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_STN_STN = combined_data.loc[combined_data['Neuron']=='STN']\n",
    "subset_STN_SNr = combined_data.loc[combined_data['Neuron']=='SNr']\n",
    "for i,population in enumerate([subset_STN_STN, subset_STN_SNr]):\n",
    "    sns.pairplot(population[['ISI_mean', 'ifr_mean', 'mean_surprise','Hemisphere']],hue='Hemisphere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(gamma='auto',random_state=42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "SVM_train_pred = svm_clf.predict(X_train)\n",
    "SVM_test_pred = svm_clf.predict(X_test)\n",
    "\n",
    "print('train precision:', precision_score(y_train,SVM_train_pred,average=None))\n",
    "print('train recall:', recall_score(y_train,SVM_train_pred,average=None))\n",
    "print('train f1:', f1_score(y_train,SVM_train_pred,average=None))\n",
    "matrix = confusion_matrix(y_train, SVM_train_pred)\n",
    "print('train accuracy:', matrix.diagonal()/matrix.sum(axis=1))\n",
    "\n",
    "print('test precision:', precision_score(y_test,SVM_test_pred,average=None))\n",
    "print('test recall:', recall_score(y_test,SVM_test_pred,average=None))\n",
    "print('test f1:', f1_score(y_test,SVM_test_pred,average=None))\n",
    "matrix = confusion_matrix(y_test, SVM_test_pred)\n",
    "print('test accuracy:', matrix.diagonal()/matrix.sum(axis=1))\n",
    "\n",
    "print('macro averaged')\n",
    "print('train precision:', precision_score(y_train,SVM_train_pred,average='macro'))\n",
    "print('train recall:', recall_score(y_train,SVM_train_pred,average='macro'))\n",
    "print('train f1:', f1_score(y_train,SVM_train_pred,average='macro'))\n",
    "print('train accuracy', accuracy_score(y_train,SVM_train_pred))\n",
    "\n",
    "print('test precision:', precision_score(y_test,SVM_test_pred,average='macro'))\n",
    "print('test recall:', recall_score(y_test,SVM_test_pred,average='macro'))\n",
    "print('test f1:', f1_score(y_test,SVM_test_pred,average='macro'))\n",
    "print('test accuracy', accuracy_score(y_test,SVM_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBDT = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.005, max_depth=None, random_state=42,verbose=2).fit(X_train, y_train)\n",
    "\n",
    "GBDT_train_pred = GBDT.predict(X_train)\n",
    "GBDT_test_pred = GBDT.predict(X_test)\n",
    "\n",
    "print('train precision:', precision_score(y_train,GBDT_train_pred,average=None))\n",
    "print('train recall:', recall_score(y_train,GBDT_train_pred,average=None))\n",
    "print('train f1:', f1_score(y_train,GBDT_train_pred,average=None))\n",
    "matrix = confusion_matrix(y_train, GBDT_train_pred)\n",
    "print('train accuracy:', matrix.diagonal()/matrix.sum(axis=1))\n",
    "\n",
    "print('test precision:', precision_score(y_test,GBDT_test_pred,average=None))\n",
    "print('test recall:', recall_score(y_test,GBDT_test_pred,average=None))\n",
    "print('test f1:', f1_score(y_test,GBDT_test_pred,average=None))\n",
    "matrix = confusion_matrix(y_test, GBDT_test_pred)\n",
    "print('test accuracy:', matrix.diagonal()/matrix.sum(axis=1))\n",
    "\n",
    "print('macro averaged')\n",
    "print('train precision:', precision_score(y_train,GBDT_train_pred,average='macro'))\n",
    "print('train recall:', recall_score(y_train,GBDT_train_pred,average='macro'))\n",
    "print('train f1:', f1_score(y_train,GBDT_train_pred,average='macro'))\n",
    "print('train accuracy', accuracy_score(y_train,GBDT_train_pred))\n",
    "\n",
    "print('test precision:', precision_score(y_test,GBDT_test_pred,average='macro'))\n",
    "print('test recall:', recall_score(y_test,GBDT_test_pred,average='macro'))\n",
    "print('test f1:', f1_score(y_test,GBDT_test_pred,average='macro'))\n",
    "print('test accuracy', accuracy_score(y_test,GBDT_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(subset_right_STN[features],\n",
    "                                                        subset_right_STN['Neuron'],\n",
    "                                                        stratify=subset_right_STN['Neuron'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "\n",
    "for i in range(1,10):\n",
    "    RF = RandomForestClassifier(n_estimators=200,random_state=18)\n",
    "    rfe = RFE(estimator=RF, n_features_to_select=i, step=1, verbose=0)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    print(rfe.get_feature_names_out())\n",
    "    # print(rfe.ranking_)\n",
    "    print(accuracy_score(y_test,rfe.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIGURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(subset_right_STN[['ISI_mean', 'ifr_mean', 'mean_surprise','Neuron','var_burst_duration']],hue='Neuron',plot_kws=dict(alpha=0.7),palette=['tab:green','tab:orange'])\n",
    "fig.savefig('Pairplot_right_STN.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(subset_right_GPi[['ISI_mean', 'ifr_mean', 'mean_surprise','Neuron','num_bursts']],hue='Neuron',plot_kws=dict(alpha=0.7),palette=['tab:blue','tab:red'])\n",
    "fig.savefig('Pairplot_right_GPi.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(subset_left_STN[['ISI_mean', 'ifr_mean', 'mean_surprise','Neuron','var_burst_duration']],hue='Neuron',plot_kws=dict(alpha=0.7),palette=['tab:green','tab:orange'])\n",
    "fig.savefig('Pairplot_left_STN.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_left_GPi_nooutlier = subset_left_GPi.loc[subset_left_GPi['outlier']==0]\n",
    "fig = sns.pairplot(subset_left_GPi_nooutlier[['ISI_mean', 'ifr_mean', 'mean_surprise','Neuron','mean_burst_duration']],hue='Neuron',plot_kws=dict(alpha=0.7),palette=['tab:blue','tab:red'])\n",
    "fig.savefig('Pairplot_left_GPi.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_min = np.mean(subset_left_GPi['mean_burst_duration']) - 2* np.std(subset_left_GPi['mean_burst_duration'])\n",
    "outlier_max = np.mean(subset_left_GPi['mean_burst_duration']) + 2* np.std(subset_left_GPi['mean_burst_duration'])\n",
    "subset_left_GPi['outlier'] = 0\n",
    "for i,val in enumerate(subset_left_GPi['mean_burst_duration']):\n",
    "    if val < outlier_min or val > outlier_max:\n",
    "        subset_left_GPi['outlier'].iloc[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "lowcut = 300  # Low cutoff frequency in Hz\n",
    "highcut = 6000  # High cutoff frequency in Hz\n",
    "fs = 12500  # Sampling frequency in Hz\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "neuron_name = 'neuron_012'\n",
    "analogsignal, spike_times, sampling_frequency, time = load_spiketrain(os.path.join(data_dir, neuron_name + '.smr'))\n",
    "filtered_data = butter_bandpass_filter(analogsignal, lowcut, highcut, fs)\n",
    "\n",
    "##### PLOT SIGNALS #####\n",
    "\n",
    "fig, ax = plt.subplots(3, sharex = True, )\n",
    "fig.suptitle(neuron_name, fontsize=16)    \n",
    "\n",
    "ax[0].plot(time,analogsignal,'tab:blue')\n",
    "ax[0].set_xlim(1,5)\n",
    "ax[0].set_ylabel('Voltage (mV)')\n",
    "\n",
    "ax[1].plot(time,filtered_data,'tab:blue', alpha=0.7)\n",
    "ax[1].set_ylabel('Voltage (mV)')\n",
    "\n",
    "ax[2].eventplot(spike_times, color='black')\n",
    "ax[2].set_xlabel(\"Time (s)\")\n",
    "fig.savefig('Example_filtering.svg')\n",
    "\n",
    "# fig, ax = plt.subplots(3, sharex = True, )\n",
    "# fig.suptitle(neuron_name, fontsize=16)    \n",
    "\n",
    "# ax[0].plot(time,analogsignal,'tab:blue')\n",
    "# ax[0].set_xlim(3,3.5)\n",
    "# ax[0].set_ylabel('Voltage')\n",
    "\n",
    "# ax[1].plot(time,filtered_data,'tab:blue', alpha=0.7)\n",
    "# ax[1].set_ylabel('Voltage')\n",
    "\n",
    "# ax[2].eventplot(spike_times, color='black')\n",
    "# ax[2].set_xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_peaks, _ = find_peaks(-filtered_data, height=-np.percentile(filtered_data,0.7))\n",
    "\n",
    "fig, ax = plt.subplots(3, sharex = True, )\n",
    "fig.suptitle(neuron_name, fontsize=16)    \n",
    "\n",
    "ax[0].plot(time,filtered_data,'tab:blue',alpha=0.7)\n",
    "ax[0].set_xlim(3,3.5)\n",
    "ax[0].set_ylabel('Voltage (mV)')\n",
    "ax[1].plot(time,filtered_data,'tab:blue',alpha=0.7)\n",
    "ax[1].scatter(spike_peaks/sampling_frequency,analogsignal[spike_peaks], color='red',s=10,alpha=0.9)\n",
    "ax[1].set_ylabel('Voltage (mV)')\n",
    "ax[2].eventplot(spike_times, color='black')\n",
    "ax[2].set_xlabel(\"Time (s)\")\n",
    "fig.savefig('Example_spiketrain.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_waveforms = np.zeros((len(spike_times),50))\n",
    "window_size = 25\n",
    "for i,peak in enumerate(spike_times):\n",
    "    spike_waveforms[i,:] = filtered_data[int(peak*12500) - window_size: int(peak*12500) + window_size]\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "pca = PCA(n_components=3)\n",
    "waveform_features = pca.fit_transform(spike_waveforms)\n",
    "\n",
    "si_score = []\n",
    "cluster_numbers = [2,3,4,5]\n",
    "for n_clusters in cluster_numbers:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(waveform_features)\n",
    "    si_score.append(silhouette_score(waveform_features, cluster_labels, metric='euclidean'))\n",
    "    \n",
    "opt_n_clusters = cluster_numbers[np.argmax(si_score)]\n",
    "print(opt_n_clusters)\n",
    "\n",
    "kmeans = KMeans(n_clusters=opt_n_clusters, random_state=0, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(waveform_features)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "scatter = ax.scatter(waveform_features[:,0], waveform_features[:,1], waveform_features[:,2],c=cluster_labels,cmap='bwr',alpha=0.6)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "ax.add_artist(legend1)\n",
    "fig.savefig('Example_neuron_sortfeatures.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,3,)\n",
    "cluster_waveforms1 = spike_waveforms[cluster_labels == 1,:]\n",
    "for spike in range(len(cluster_waveforms1)):\n",
    "    axes[0].plot([i for i in np.linspace(-2,2,50)],cluster_waveforms1[spike,:],alpha=0.05,color = 'blue',linewidth=1)\n",
    "axes[0].plot([i for i in np.linspace(-2,2,50)],np.mean(cluster_waveforms1,axis=0), color='blue',linewidth=2,alpha=0.8)\n",
    "axes[0].set_box_aspect(1)\n",
    "\n",
    "cluster_waveforms2 = spike_waveforms[cluster_labels == 0,:]\n",
    "for spike in range(len(cluster_waveforms2)):\n",
    "     axes[1].plot([i for i in np.linspace(-2,2,50)],cluster_waveforms2[spike,:],alpha=0.05,color = 'red',linewidth=1)\n",
    "axes[1].plot([i for i in np.linspace(-2,2,50)],np.mean(cluster_waveforms2,axis=0), color='red',linewidth=2,alpha=0.8)\n",
    "axes[1].set_box_aspect(1)\n",
    "axes[0].set_title('Cluster 1')\n",
    "axes[1].set_title('Cluster 2')\n",
    "\n",
    "for spike in range(len(spike_waveforms)):\n",
    "     axes[2].plot([i for i in np.linspace(-2,2,50)],spike_waveforms[spike,:],alpha=0.05,color = 'black',linewidth=1)\n",
    "axes[2].plot([i for i in np.linspace(-2,2,50)],np.mean(spike_waveforms,axis=0), color='black',linewidth=2,alpha=0.8)\n",
    "axes[2].set_box_aspect(1)\n",
    "\n",
    "fig.supxlabel('Time (ms)')\n",
    "fig.supylabel('Voltage (mV)')\n",
    "fig.tight_layout()\n",
    "fig.savefig('Example_cluster.svg')\n",
    "# for spike in range(len(spike_waveforms)):\n",
    "#     plt.plot(spike_waveforms[spike,:],alpha=0.1,color='tab:blue',linewidth = 1)\n",
    "    \n",
    "# plt.plot(np.mean(spike_waveforms,axis=0),color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=25\n",
    "snr_ls = []\n",
    "isi_isol_ls = []\n",
    "mean_isi_ls = []\n",
    "num_refrac_ls = [] \n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.smr'):\n",
    "        analogsignal, spike_times, sampling_frequency, time = load_spiketrain(os.path.join(data_dir,file))\n",
    "        waveforms = np.zeros((len(spike_times), window*2))\n",
    "        for i, spike in enumerate(spike_times):\n",
    "            if spike*sampling_frequency - window >= 0 and spike*sampling_frequency + window <= len(analogsignal):\n",
    "                waveforms[i,:] = analogsignal[int(spike*sampling_frequency)-window:int(spike*sampling_frequency)+window]\n",
    "        \n",
    "        waveforms = waveforms[~np.all(waveforms == 0, axis=1)]\n",
    "        mean_waveform = np.mean(waveforms,axis=0)\n",
    "        snr = (np.max(mean_waveform) - np.min(mean_waveform) )/ (np.std(mean_waveform) * 2)\n",
    "        snr_ls.append(snr)\n",
    "        \n",
    "        ISI = np.diff(spike_times)\n",
    "        ISI_cv = np.std(ISI) / np.mean(ISI)\n",
    "        isi_isol_ls.append(1/ISI_cv)\n",
    "        mean_isi_ls.append(np.mean(ISI))\n",
    "        \n",
    "        num_refrac_ls.append(len(ISI[ISI<=0.001]))\n",
    "\n",
    "fig,ax=plt.subplots(1,2)        \n",
    "sns.histplot(snr_ls,ax=ax[0])\n",
    "sns.histplot(isi_isol_ls,ax=ax[1])\n",
    "# sns.histplot(num_refrac_ls,ax=ax[2])\n",
    "# ax[1].axvline(x=0.001)\n",
    "# ax[1].set_xlim(0,0.1)\n",
    "\n",
    "ax[0].set_xlabel('Mean signal-to-noise ratio')\n",
    "ax[1].set_xlabel('ISI isolation')\n",
    "\n",
    "ax[0].set_box_aspect(1)\n",
    "ax[1].set_box_aspect(1)\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig('spikesort_metrics.svg')\n",
    "\n",
    "print('mean snr', np.mean(snr_ls))\n",
    "print('isolation ', np.mean(isi_isol_ls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
